{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import kaleido\n",
    "# Hugging Face Transformers and datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# scikit-learn for dataset splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this first cell is for loading my dataset, splitting it into a test and training set. then saving the respective datasets to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mediabiasgroup/BABE-v3\")\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "DF_TRAIN, DF_TEST = train_test_split(df, test_size=0.20, random_state=42)\n",
    "DF_TRAIN.to_csv(\"TRAINING_DATAFRAME.csv\", index=False)\n",
    "DF_TEST.to_csv(\"TESTING_DATAFRAME.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for running the pre trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for the analysis of the D1V1DE bias-detection model (https://huggingface.co/D1V1DE/bias-detection?text=I+like+you.+I+love+you)\n",
    "def PRED1V1DE():\n",
    "    try:\n",
    "        pipe = pipeline(\"text-classification\", model=\"D1V1DE/bias-detection\")\n",
    "        CurrentDF = pd.read_csv(\"TESTING_DATAFRAME.csv\")\n",
    "        CurrentDF['Predicted'] = 'XXX'\n",
    "        CurrentDF.drop(['news_link','outlet','label','label_opinion','biased_words'], axis=1, inplace=True)\n",
    "        for index, row in CurrentDF.iterrows():\n",
    "            text_data = row['text']\n",
    "            bias = pipe(text_data)\n",
    "            CurrentDF.at[index, 'Predicted'] = bias[0][\"label\"]\n",
    "        CurrentDF.to_csv(\"temp.csv\", index = False)\n",
    "        \n",
    "        Type =  ScorePerTopic(CurrentDF, \"type\")\n",
    "        Topic = ScorePerTopic(CurrentDF, \"topic\")\n",
    "        results = Type.join(Topic).T\n",
    "        results.to_csv(\"test.csv\", index=True)\n",
    "        print(\"D1V1DE/bias-detection examination completed successfully\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(\"D1V1DE/bias-detection failed\")\n",
    "        print(e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the bellow section is for fine tuning models and their evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#D1V1DE/bias-detection fine tuning and analysis\n",
    "\n",
    "#fine tuning the model with my training dataset\n",
    "def FIND1V1DE(training_data_path):\n",
    "    # Load and preprocess the dataset\n",
    "    df = pd.read_csv(training_data_path)\n",
    "    df = df[['text', 'label']]\n",
    "    df_train, df_val = train_test_split(df, test_size=0.1)\n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    val_dataset = Dataset.from_pandas(df_val)\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    # Load the pretrained model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10\n",
    "    )\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset\n",
    "    )\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(\"./fine_tuned/D1V1DE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for fine-tuning individual models\n",
    "#FIND1V1DE(\"TRAINING_DATAFRAME.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is all nececary code for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for doing all my scoring and accuracy testing of models using the bias dataset\n",
    "#variables need renaming and potentailly this code could be sped up\n",
    "def ScorePerTopic(data, field):\n",
    "    unique_items = data[field].unique().tolist()\n",
    "    scores = {item: {\"correct\": 0, \"count\": 0} for item in unique_items}\n",
    "    for index, row in data.iterrows():\n",
    "        correct = False\n",
    "        if pd.isna(row[field]) and row['Predicted'] == 'NEUTRAL':\n",
    "            correct = True\n",
    "        elif not pd.isna(row[field]) and row['Predicted'] != 'NEUTRAL':\n",
    "            correct = True\n",
    "        scores[row[field]]['count'] += 1\n",
    "        if correct:\n",
    "            scores[row[field]]['correct'] += 1\n",
    "    current = {}\n",
    "    for item in scores:\n",
    "        scores[item]['score'] = scores[item]['correct'] / scores[item]['count']\n",
    "    scores = pd.DataFrame(scores).T\n",
    "    scores[\"field\"] = field #could do with changing from field\n",
    "    return pd.DataFrame(scores).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_dataset):\n",
    "    # Initialize the trainer\n",
    "    trainer = Trainer(model=model)\n",
    "    # Tokenize the evaluation dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate(tokenized_eval_dataset)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the standard python tools for evaluating models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscar/miniforge3/envs/oscar/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 9960.77 examples/s]\n",
      "100%|██████████| 104/104 [00:17<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIND1V1DE:  {'eval_loss': 0.38539549708366394, 'eval_runtime': 17.0939, 'eval_samples_per_second': 48.263, 'eval_steps_per_second': 6.084}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 825/825 [00:00<00:00, 10552.01 examples/s]\n",
      "100%|██████████| 104/104 [00:16<00:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED1V1DE:  {'eval_loss': 0.29630759358406067, 'eval_runtime': 16.4991, 'eval_samples_per_second': 50.003, 'eval_steps_per_second': 6.303}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#evalating fine tuned D1V1DE model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"fine_tuned/D1V1DE\")\n",
    "df_eval = pd.read_csv('TESTING_DATAFRAME.csv')\n",
    "eval_dataset = Dataset.from_pandas(df_eval[['text', 'label']])\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluate_model(model, tokenizer, eval_dataset)\n",
    "\n",
    "print(\"FIND1V1DE: \", evaluation_results)\n",
    "\n",
    "\n",
    "# Replace 'D1V1DE/original-model-name' with the correct path to the original model on Hugging Face\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "# Evaluate the original model\n",
    "original_evaluation_results = evaluate_model(original_model, tokenizer, eval_dataset)\n",
    "\n",
    "# Print the evaluation results of the original model\n",
    "print(\"PRED1V1DE: \", original_evaluation_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bellow is code for visualisations and evaluation assistive tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def star(data,key):\n",
    "    FilteredData = data[data['field'] == key]\n",
    "    # Preparing the data for the star plot (radar chart)\n",
    "    FilteredData = FilteredData.reset_index()\n",
    "    FilteredData['index'] = FilteredData['index'].fillna('No Bias')\n",
    "    labels=FilteredData['index']\n",
    "    #print(labels)\n",
    "    stats=FilteredData['score']\n",
    "    #print(stats)\n",
    "\n",
    "    # Create radar chart\n",
    "    angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
    "    stats=np.concatenate((stats,[stats[0]]))\n",
    "    angles+=angles[:1]\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "    ax.fill(angles, stats, color='blue', alpha=0.25)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.title('Star Plot of '+  str(key) +' vs. accuracy rating')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oscar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78d4af1728f09eeb9c0c57e976bf7f00c36f6b97261929e2a1d74285332f4bc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
