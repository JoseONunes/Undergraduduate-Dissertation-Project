{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import kaleido\n",
    "# Hugging Face Transformers and datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "# scikit-learn for dataset splitting\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this first cell is for loading my dataset, splitting it into a test and training set. then saving the respective datasets to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mediabiasgroup/BABE-v3 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/oscar/.cache/huggingface/datasets/mediabiasgroup___BABE-v3/default/0.0.0/5493fe1686f29d4fce6912ccf9e2e03780493bd6 (last modified on Mon Mar 18 10:23:39 2024).\n"
     ]
    }
   ],
   "source": [
    "def IMPORT_BABEv3():\n",
    "    dataset = load_dataset(\"mediabiasgroup/BABE-v3\")\n",
    "    df = pd.DataFrame(dataset[\"train\"])\n",
    "    DF_TRAIN, DF_TEST = train_test_split(df, test_size=0.20, random_state=42)\n",
    "    DF_TRAIN.to_csv(\"TRAINING_DATAFRAME.csv\", index=False)\n",
    "    DF_TEST.to_csv(\"TESTING_DATAFRAME.csv\", index=False)\n",
    "\n",
    "IMPORT_BABEv3()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for running the pre trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1V1DE/bias-detection examination completed successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>count</th>\n",
       "      <th>score</th>\n",
       "      <th>field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>133.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>46.020761</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>64.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>31.527094</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>center</th>\n",
       "      <td>122.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>89.705882</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>66.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>33.502538</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white-nationalism</th>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>13.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>59.090909</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#metoo</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marriage-equality</th>\n",
       "      <td>20.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taxes</th>\n",
       "      <td>29.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>53.703704</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student-debt</th>\n",
       "      <td>7.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>31.818182</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black lives matter</th>\n",
       "      <td>30.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment</th>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>islam</th>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>51.428571</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abortion</th>\n",
       "      <td>17.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>36.956522</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blm</th>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>85.294118</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>international-politics-and-world-news</th>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>73.076923</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gun-control</th>\n",
       "      <td>22.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>62.857143</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>universal health care</th>\n",
       "      <td>22.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>46.808511</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immigration</th>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.235294</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elections-2020</th>\n",
       "      <td>13.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>44.827586</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccine</th>\n",
       "      <td>36.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>54.545455</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccines</th>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.434783</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gun control</th>\n",
       "      <td>23.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>43.396226</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump-presidency</th>\n",
       "      <td>7.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>25.925926</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>middle-class</th>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>72.413793</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>12.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      correct  count      score  field\n",
       "NaN                                     133.0  289.0  46.020761   type\n",
       "left                                     64.0  203.0  31.527094   type\n",
       "center                                  122.0  136.0  89.705882   type\n",
       "right                                    66.0  197.0  33.502538   type\n",
       "white-nationalism                         6.0   30.0       20.0  topic\n",
       "gender                                   13.0   22.0  59.090909  topic\n",
       "#metoo                                    6.0    7.0  85.714286  topic\n",
       "marriage-equality                        20.0   68.0  29.411765  topic\n",
       "taxes                                    29.0   54.0  53.703704  topic\n",
       "student-debt                              7.0   22.0  31.818182  topic\n",
       "black lives matter                       30.0   50.0       60.0  topic\n",
       "environment                              20.0   40.0       50.0  topic\n",
       "islam                                    18.0   35.0  51.428571  topic\n",
       "abortion                                 17.0   46.0  36.956522  topic\n",
       "blm                                      29.0   34.0  85.294118  topic\n",
       "international-politics-and-world-news    19.0   26.0  73.076923  topic\n",
       "gun-control                              22.0   35.0  62.857143  topic\n",
       "universal health care                    22.0   47.0  46.808511  topic\n",
       "immigration                              13.0   34.0  38.235294  topic\n",
       "elections-2020                           13.0   29.0  44.827586  topic\n",
       "vaccine                                  36.0   66.0  54.545455  topic\n",
       "vaccines                                  7.0   23.0  30.434783  topic\n",
       "gun control                              23.0   53.0  43.396226  topic\n",
       "trump-presidency                          7.0   27.0  25.925926  topic\n",
       "middle-class                             18.0   27.0  66.666667  topic\n",
       "sport                                    21.0   29.0  72.413793  topic\n",
       "coronavirus                              12.0   21.0  57.142857  topic"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is for the analysis of the D1V1DE bias-detection model (https://huggingface.co/D1V1DE/bias-detection?text=I+like+you.+I+love+you)\n",
    "def PRE_TEST_D1V1DE():\n",
    "    try:\n",
    "        pipe = pipeline(\"text-classification\", model=\"D1V1DE/bias-detection\")\n",
    "        CurrentDF = pd.read_csv(\"TESTING_DATAFRAME.csv\")\n",
    "        CurrentDF['Predicted'] = 'XXX'\n",
    "        CurrentDF.drop(['news_link','outlet','label','label_opinion','biased_words'], axis=1, inplace=True)\n",
    "        for index, row in CurrentDF.iterrows():\n",
    "            text_data = row['text']\n",
    "            bias = pipe(text_data)\n",
    "            CurrentDF.at[index, 'Predicted'] = bias[0][\"label\"]\n",
    "        CurrentDF.to_csv(\"temp.csv\", index = False)\n",
    "        \n",
    "        Type =  GEN_EVAL_ScorePerTopic(CurrentDF, \"type\")\n",
    "        Topic = GEN_EVAL_ScorePerTopic(CurrentDF, \"topic\")\n",
    "        results = Type.join(Topic).T\n",
    "        results['score'] = results['score'] * 100\n",
    "        results.to_csv(\"PRED1V1DE.csv\", index=True)\n",
    "        print(\"D1V1DE/bias-detection examination completed successfully\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(\"D1V1DE/bias-detection failed\")\n",
    "        print(e)\n",
    "\n",
    "PRE_TEST_D1V1DE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned D1V1DE/bias-detection model evaluation completed successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>count</th>\n",
       "      <th>score</th>\n",
       "      <th>field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>119.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>41.176471</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>85.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>41.871921</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>center</th>\n",
       "      <td>128.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>94.117647</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>83.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>42.13198</td>\n",
       "      <td>type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white-nationalism</th>\n",
       "      <td>8.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#metoo</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marriage-equality</th>\n",
       "      <td>22.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>32.352941</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taxes</th>\n",
       "      <td>30.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>student-debt</th>\n",
       "      <td>12.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>54.545455</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black lives matter</th>\n",
       "      <td>34.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment</th>\n",
       "      <td>27.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>67.5</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>islam</th>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abortion</th>\n",
       "      <td>21.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>45.652174</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blm</th>\n",
       "      <td>32.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>94.117647</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>international-politics-and-world-news</th>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>80.769231</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gun-control</th>\n",
       "      <td>22.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>62.857143</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>universal health care</th>\n",
       "      <td>23.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>48.93617</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immigration</th>\n",
       "      <td>16.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>47.058824</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elections-2020</th>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>55.172414</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccine</th>\n",
       "      <td>39.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>59.090909</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccines</th>\n",
       "      <td>11.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>47.826087</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gun control</th>\n",
       "      <td>26.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>49.056604</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trump-presidency</th>\n",
       "      <td>11.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>40.740741</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>middle-class</th>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>70.37037</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>22.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>75.862069</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      correct  count      score  field\n",
       "NaN                                     119.0  289.0  41.176471   type\n",
       "left                                     85.0  203.0  41.871921   type\n",
       "center                                  128.0  136.0  94.117647   type\n",
       "right                                    83.0  197.0   42.13198   type\n",
       "white-nationalism                         8.0   30.0  26.666667  topic\n",
       "gender                                   14.0   22.0  63.636364  topic\n",
       "#metoo                                    7.0    7.0      100.0  topic\n",
       "marriage-equality                        22.0   68.0  32.352941  topic\n",
       "taxes                                    30.0   54.0  55.555556  topic\n",
       "student-debt                             12.0   22.0  54.545455  topic\n",
       "black lives matter                       34.0   50.0       68.0  topic\n",
       "environment                              27.0   40.0       67.5  topic\n",
       "islam                                    15.0   35.0  42.857143  topic\n",
       "abortion                                 21.0   46.0  45.652174  topic\n",
       "blm                                      32.0   34.0  94.117647  topic\n",
       "international-politics-and-world-news    21.0   26.0  80.769231  topic\n",
       "gun-control                              22.0   35.0  62.857143  topic\n",
       "universal health care                    23.0   47.0   48.93617  topic\n",
       "immigration                              16.0   34.0  47.058824  topic\n",
       "elections-2020                           16.0   29.0  55.172414  topic\n",
       "vaccine                                  39.0   66.0  59.090909  topic\n",
       "vaccines                                 11.0   23.0  47.826087  topic\n",
       "gun control                              26.0   53.0  49.056604  topic\n",
       "trump-presidency                         11.0   27.0  40.740741  topic\n",
       "middle-class                             19.0   27.0   70.37037  topic\n",
       "sport                                    22.0   29.0  75.862069  topic\n",
       "coronavirus                              18.0   21.0  85.714286  topic"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is for the analysis of the finetuned D1V1DE bias-detection model (https://huggingface.co/D1V1DE/bias-detection?text=I+like+you.+I+love+you)\n",
    "def FIN_TEST_D1V1DE():\n",
    "    try:\n",
    "        # Load the fine-tuned model as a pipeline for text classification\n",
    "        pipe = pipeline(\"text-classification\", model=\"./fine_tuned/D1V1DE\")\n",
    "        # Load the test dataset\n",
    "        test_df = pd.read_csv(\"TESTING_DATAFRAME.csv\")\n",
    "        test_df['Predicted'] = 'XXX'  # Initialize the Predicted column\n",
    "        # Drop unnecessary columns\n",
    "        test_df.drop(['news_link', 'outlet', 'label', 'label_opinion', 'biased_words'], axis=1, inplace=True)\n",
    "        # Make predictions and store in the DataFrame\n",
    "        for index, row in test_df.iterrows():\n",
    "            text_data = row['text']\n",
    "            prediction = pipe(text_data)\n",
    "            test_df.at[index, 'Predicted'] = prediction[0][\"label\"]\n",
    "\n",
    "        # Save the DataFrame with predictions to a temporary CSV file\n",
    "        test_df.to_csv(\"temp_predictions.csv\", index=False)\n",
    "\n",
    "        # Analyze results by type and topic\n",
    "        Type = GEN_EVAL_ScorePerTopic(test_df, \"type\")\n",
    "        Topic = GEN_EVAL_ScorePerTopic(test_df, \"topic\")\n",
    "\n",
    "        # Combine results and calculate scores\n",
    "        results = Type.join(Topic).T\n",
    "        results['score'] = results['score'] * 100\n",
    "\n",
    "        # Save final results to a CSV file\n",
    "        results.to_csv(\"FIN_TEST_D1V1DE_RESULTS.csv\", index=True)\n",
    "        print(\"Fine-tuned D1V1DE/bias-detection model evaluation completed successfully\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(\"Evaluation of the fine-tuned D1V1DE/bias-detection model failed\")\n",
    "        print(e)\n",
    "\n",
    "# Example usage\n",
    "FIN_TEST_D1V1DE()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the bellow section is for fine tuning models and their evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2966/2966 [00:00<00:00, 10660.06 examples/s]\n",
      "Map: 100%|██████████| 330/330 [00:00<00:00, 10420.63 examples/s]\n",
      "/Users/oscar/miniforge3/envs/oscar/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "  2%|▏         | 10/558 [00:11<10:27,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3819, 'grad_norm': 24.358970642089844, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 20/558 [00:23<10:12,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3204, 'grad_norm': 23.58262825012207, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 30/558 [00:34<09:58,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3584, 'grad_norm': 19.62409210205078, 'learning_rate': 3e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 40/558 [00:45<09:47,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2999, 'grad_norm': 11.214188575744629, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 50/558 [00:57<09:36,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2756, 'grad_norm': 21.374582290649414, 'learning_rate': 5e-06, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 60/558 [01:08<09:22,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3748, 'grad_norm': 3.4241037368774414, 'learning_rate': 6e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 70/558 [01:20<09:34,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4317, 'grad_norm': 14.77900505065918, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 80/558 [01:31<09:19,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4097, 'grad_norm': 20.95553207397461, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 90/558 [01:43<09:06,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3931, 'grad_norm': 13.118010520935059, 'learning_rate': 9e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/558 [01:55<08:46,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4772, 'grad_norm': 33.427513122558594, 'learning_rate': 1e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 110/558 [02:06<08:34,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3196, 'grad_norm': 18.216777801513672, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 120/558 [02:18<08:21,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.36, 'grad_norm': 26.892658233642578, 'learning_rate': 1.2e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 130/558 [02:29<08:21,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.419, 'grad_norm': 20.00519561767578, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 140/558 [02:41<07:58,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3293, 'grad_norm': 29.422935485839844, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 150/558 [02:52<07:50,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3562, 'grad_norm': 20.47928810119629, 'learning_rate': 1.5e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 160/558 [03:04<07:39,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5021, 'grad_norm': 34.013729095458984, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 170/558 [03:15<07:27,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4971, 'grad_norm': 11.920598030090332, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 180/558 [03:27<07:16,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4027, 'grad_norm': 13.094677925109863, 'learning_rate': 1.8e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 190/558 [03:38<07:16,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3508, 'grad_norm': 11.992867469787598, 'learning_rate': 1.9e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 200/558 [03:51<07:36,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2718, 'grad_norm': 20.38723373413086, 'learning_rate': 2e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 210/558 [04:04<07:39,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2007, 'grad_norm': 23.12297248840332, 'learning_rate': 2.1e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 220/558 [04:17<07:26,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5016, 'grad_norm': 21.839778900146484, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 230/558 [04:31<07:11,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3398, 'grad_norm': 12.050722122192383, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 240/558 [04:44<06:56,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1754, 'grad_norm': 1.7716572284698486, 'learning_rate': 2.4e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 250/558 [04:57<06:50,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2187, 'grad_norm': 15.061874389648438, 'learning_rate': 2.5e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 260/558 [05:10<06:32,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4196, 'grad_norm': 15.103972434997559, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 270/558 [05:24<06:20,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3582, 'grad_norm': 16.9282169342041, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 280/558 [05:37<06:05,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3379, 'grad_norm': 15.1163911819458, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 290/558 [05:50<05:49,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3512, 'grad_norm': 10.724246978759766, 'learning_rate': 2.9e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 300/558 [06:03<05:33,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3004, 'grad_norm': 29.348575592041016, 'learning_rate': 3e-05, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 310/558 [06:16<05:18,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3169, 'grad_norm': 16.09638786315918, 'learning_rate': 3.1e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 320/558 [06:29<05:16,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3088, 'grad_norm': 13.173498153686523, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 330/558 [06:42<04:54,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2892, 'grad_norm': 10.872442245483398, 'learning_rate': 3.3e-05, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 340/558 [06:55<04:49,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3727, 'grad_norm': 11.119922637939453, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 350/558 [07:09<04:32,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4171, 'grad_norm': 30.46381378173828, 'learning_rate': 3.5e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 360/558 [07:22<04:14,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5223, 'grad_norm': 10.132445335388184, 'learning_rate': 3.6e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 370/558 [07:35<04:01,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4232, 'grad_norm': 11.994037628173828, 'learning_rate': 3.7e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 380/558 [07:47<03:50,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2505, 'grad_norm': 14.553853988647461, 'learning_rate': 3.8e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 390/558 [08:00<03:36,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2758, 'grad_norm': 17.802513122558594, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 400/558 [08:13<03:25,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1984, 'grad_norm': 15.71505355834961, 'learning_rate': 4e-05, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 410/558 [08:25<03:05,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2327, 'grad_norm': 16.402812957763672, 'learning_rate': 4.1e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 420/558 [08:38<02:59,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2622, 'grad_norm': 22.064594268798828, 'learning_rate': 4.2e-05, 'epoch': 2.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 430/558 [08:52<02:46,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4405, 'grad_norm': 82.52388763427734, 'learning_rate': 4.3e-05, 'epoch': 2.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 440/558 [09:05<02:35,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5406, 'grad_norm': 38.06264877319336, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 450/558 [09:18<02:22,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2771, 'grad_norm': 12.347790718078613, 'learning_rate': 4.5e-05, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 460/558 [09:31<02:04,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3268, 'grad_norm': 27.564191818237305, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 470/558 [09:44<01:53,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2239, 'grad_norm': 35.727149963378906, 'learning_rate': 4.7e-05, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 480/558 [09:57<01:42,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4671, 'grad_norm': 38.475040435791016, 'learning_rate': 4.8e-05, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 490/558 [10:10<01:29,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3073, 'grad_norm': 11.285598754882812, 'learning_rate': 4.9e-05, 'epoch': 2.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 500/558 [10:23<01:14,  1.29s/it]Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2544, 'grad_norm': 31.389554977416992, 'learning_rate': 5e-05, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 510/558 [10:36<01:01,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2774, 'grad_norm': 20.52629280090332, 'learning_rate': 4.1379310344827587e-05, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 520/558 [10:50<00:50,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3328, 'grad_norm': 10.32570743560791, 'learning_rate': 3.275862068965517e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 530/558 [11:03<00:36,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2875, 'grad_norm': 14.379392623901367, 'learning_rate': 2.413793103448276e-05, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 540/558 [11:16<00:23,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2425, 'grad_norm': 23.04095458984375, 'learning_rate': 1.5517241379310346e-05, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 550/558 [11:29<00:10,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2372, 'grad_norm': 29.879043579101562, 'learning_rate': 6.896551724137932e-06, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 558/558 [11:38<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 698.96, 'train_samples_per_second': 12.73, 'train_steps_per_second': 0.798, 'train_loss': 0.33937042673856127, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "#fine tuning the model with my training dataset\n",
    "def FIN_TRAIN_D1V1DE(training_data_path):\n",
    "    # Load and preprocess the dataset\n",
    "    df = pd.read_csv(training_data_path)\n",
    "    df = df[['text', 'label']]\n",
    "    df_train, df_val = train_test_split(df, test_size=0.1)\n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    val_dataset = Dataset.from_pandas(df_val)\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    # Load the pretrained model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10\n",
    "    )\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset\n",
    "    )\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    # Save the fine-tuned model\n",
    "    tokenizer.save_pretrained(\"./fine_tuned/D1V1DE\")\n",
    "    model.save_pretrained(\"./fine_tuned/D1V1DE\")\n",
    "\n",
    "\n",
    "#code for fine-tuning individual models\n",
    "#FIN_TRAIN_D1V1DE(\"TRAINING_DATAFRAME.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is all nececary code for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for doing all my scoring and accuracy testing of models using the bias dataset\n",
    "#variables need renaming and potentailly this code could be sped up\n",
    "def GEN_EVAL_ScorePerTopic(data, field):\n",
    "    unique_items = data[field].unique().tolist()\n",
    "    scores = {item: {\"correct\": 0, \"count\": 0} for item in unique_items}\n",
    "    for index, row in data.iterrows():\n",
    "        correct = False\n",
    "        if pd.isna(row[field]) and row['Predicted'] == 'NEUTRAL':\n",
    "            correct = True\n",
    "        elif not pd.isna(row[field]) and row['Predicted'] != 'NEUTRAL':\n",
    "            correct = True\n",
    "        scores[row[field]]['count'] += 1\n",
    "        if correct:\n",
    "            scores[row[field]]['correct'] += 1\n",
    "    current = {}\n",
    "    for item in scores:\n",
    "        scores[item]['score'] = scores[item]['correct'] / scores[item]['count']\n",
    "    scores = pd.DataFrame(scores).T\n",
    "    scores[\"field\"] = field #could do with changing from field\n",
    "    return pd.DataFrame(scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GEN_EVAL_evaluate_model(model, tokenizer, eval_dataset):\n",
    "    #used for evaluating performance of models in terms of speed. needs further research\n",
    "    # Initialize the trainer\n",
    "    trainer = Trainer(model=model)\n",
    "    # Tokenize the evaluation dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate(tokenized_eval_dataset)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the standard python tools for evaluating models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscar/miniforge3/envs/oscar/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 825/825 [00:00<00:00, 7863.52 examples/s]\n",
      "100%|██████████| 104/104 [00:16<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIND1V1DE:  {'eval_loss': 0.4441041946411133, 'eval_runtime': 16.5461, 'eval_samples_per_second': 49.861, 'eval_steps_per_second': 6.285}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 825/825 [00:00<00:00, 9296.54 examples/s]\n",
      "100%|██████████| 104/104 [00:16<00:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED1V1DE:  {'eval_loss': 0.29630759358406067, 'eval_runtime': 16.5132, 'eval_samples_per_second': 49.96, 'eval_steps_per_second': 6.298}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def EVAL_D1V1DE_evaluate_model():\n",
    "    #evalating fine tuned D1V1DE model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"fine_tuned/D1V1DE\")\n",
    "    df_eval = pd.read_csv('TESTING_DATAFRAME.csv')\n",
    "    eval_dataset = Dataset.from_pandas(df_eval[['text', 'label']])\n",
    "    # Evaluate the model\n",
    "    evaluation_results = GEN_EVAL_evaluate_model(model, tokenizer, eval_dataset)\n",
    "\n",
    "    print(\"FIND1V1DE: \", evaluation_results)\n",
    "\n",
    "\n",
    "    # Replace 'D1V1DE/original-model-name' with the correct path to the original model on Hugging Face\n",
    "    original_model = AutoModelForSequenceClassification.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "    # Evaluate the original model\n",
    "    original_evaluation_results = GEN_EVAL_evaluate_model(original_model, tokenizer, eval_dataset)\n",
    "\n",
    "    # Print the evaluation results of the original model\n",
    "    print(\"PRED1V1DE: \", original_evaluation_results)\n",
    "\n",
    "EVAL_D1V1DE_evaluate_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bellow is code for visualisations and evaluation assistive tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GEN_VIS_star(data,key):\n",
    "    FilteredData = data[data['field'] == key]\n",
    "    # Preparing the data for the star plot (radar chart)\n",
    "    FilteredData = FilteredData.reset_index()\n",
    "    FilteredData['index'] = FilteredData['index'].fillna('No Bias')\n",
    "    labels=FilteredData['index']\n",
    "    #print(labels)\n",
    "    stats=FilteredData['score']\n",
    "    #print(stats)\n",
    "\n",
    "    # Create radar chart\n",
    "    angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
    "    stats=np.concatenate((stats,[stats[0]]))\n",
    "    angles+=angles[:1]\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "    ax.fill(angles, stats, color='blue', alpha=0.25)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.title('Star Plot of '+  str(key) +' vs. accuracy rating')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oscar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78d4af1728f09eeb9c0c57e976bf7f00c36f6b97261929e2a1d74285332f4bc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
