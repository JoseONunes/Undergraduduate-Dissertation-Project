{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_metric' from 'datasets' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     AutoTokenizer,\n\u001b[1;32m     22\u001b[0m     AutoModelForSequenceClassification,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     logging \u001b[38;5;28;01mas\u001b[39;00m hf_logging\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, Dataset, load_metric\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Logging and warnings management\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_metric' from 'datasets' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Basic data handling and array manipulations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "# Machine learning, data splitting, and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# TensorFlow and PyTorch for neural networks\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# Hugging Face Transformers and Datasets for NLP tasks and data handling\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    AutoConfig,\n",
    "    DataCollatorWithPadding,\n",
    "    logging as hf_logging\n",
    ")\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "\n",
    "# Logging and warnings management\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing files already exist. Skipping processing.\n",
      "Data loaded.\n",
      "Intermediate results for index 0: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 1: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 2: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 3: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 4: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 5: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 6: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 7: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 8: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 9: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 10: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 11: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 12: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 13: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 14: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 15: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 16: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 17: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 18: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 19: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 20: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 21: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 22: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 23: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 24: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 25: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 26: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 27: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 28: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 29: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 30: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 31: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 32: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 33: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 34: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 35: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 36: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 37: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 38: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 39: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 40: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 41: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 42: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 43: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 44: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 45: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 46: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 47: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 48: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 49: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 50: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 51: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 52: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 53: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 54: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 55: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 56: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 57: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 58: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 59: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 60: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 61: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 62: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 63: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 64: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 65: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 66: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 67: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 68: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 69: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 70: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 71: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 72: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 73: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 74: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 75: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 76: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 77: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 78: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 79: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 80: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 81: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 82: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 83: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 84: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 85: ['biased', 'biased', 'neutral']\n",
      "Intermediate results for index 86: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 87: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 88: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 89: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 90: ['neutral', 'biased', 'biased']\n",
      "Intermediate results for index 91: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 92: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 93: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 94: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 95: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 96: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 97: ['biased', 'biased', 'biased']\n",
      "Intermediate results for index 98: ['neutral', 'biased', 'neutral']\n",
      "Intermediate results for index 99: ['biased', 'biased', 'biased']\n",
      "Data processing complete and saved.\n"
     ]
    }
   ],
   "source": [
    "def IMPORT_BABEv3():\n",
    "    train_path = \"./clean_datasets/TRAINING_DATAFRAME.csv\"\n",
    "    test_path = \"./clean_datasets/TESTING_DATAFRAME.csv\"\n",
    "    # Check if the files already exist\n",
    "    if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "        print(\"Training and testing files already exist. Skipping processing.\")\n",
    "        return \n",
    "    dataset = load_dataset(\"mediabiasgroup/BABE-v3\")\n",
    "    df = pd.DataFrame(dataset[\"train\"])\n",
    "    df['Predicted'] = 'XXX'\n",
    "    df.drop(['news_link','outlet','label_opinion','biased_words'], axis=1, inplace=True)\n",
    "    df['label'] = 0\n",
    "    df['label'] = df['type'].isin(['left', 'right', 'center']).astype(int)\n",
    "    DF_TRAIN, DF_TEST = train_test_split(df, test_size=0.20, random_state=42)\n",
    "    DF_TRAIN.to_csv(\"./clean_datasets/TRAINING_DATAFRAME.csv\", index=False)\n",
    "    DF_TEST.to_csv(\"./clean_datasets/TESTING_DATAFRAME.csv\", index=False)\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "def IMPORT_ALL_THE_NEWS_1():\n",
    "    # Define the output file path\n",
    "    output_path = \"./clean_datasets/BIG_TESTING_DATAFRAME.csv\"\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(\"Processed file already exists. Skipping processing.\")\n",
    "        return\n",
    "    # Load the data\n",
    "    df = pd.read_csv('./unclean_datasets/ALL_THE_NEWS_1.csv')\n",
    "    print(\"Data loaded.\")\n",
    "    df = df.iloc[:100]  # Limiting the dataset for demonstration\n",
    "    df.drop(['id', 'title', 'publication', 'author', 'date', 'year', 'month', 'url', 'Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df.rename(columns={'content': 'text'})\n",
    "    df[\"label\"] = \"unclassified\"  # Initial label before classification\n",
    "\n",
    "    # Setup model pipelines outside the loop for efficiency\n",
    "    max_length = 300  # Based on typical model max token lengths\n",
    "    model_names = [\"d4data/bias-detection-model\", \"D1V1DE/bias-detection\", \"valurank/distilroberta-bias\"]\n",
    "    model_names = [\"./models/D1V1DE-on-BABE-on-PRANJALI/\", \"./models/D4DATA-on-BABE/\",\"./models/VALURANK-on-BABE-on-PRANJALI/\"]\n",
    "    pipelines = {name: pipeline(\"text-classification\", model=name) for name in model_names}\n",
    "    # Process each row\n",
    "    for index, row in df.iterrows():\n",
    "        text_data = row['text']\n",
    "        words = text_data.split()\n",
    "        chunk_size = int(max_length / 2)\n",
    "        chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "        bias_results = []\n",
    "        \n",
    "        for name, pipe in pipelines.items():\n",
    "            chunk_predictions = [pipe(chunk)[0].get(\"label\", \"No Label\").lower() for chunk in chunks]\n",
    "            counter = Counter(chunk_predictions)\n",
    "            bias_results.append(counter.most_common(1)[0][0])\n",
    "        print(f\"Intermediate results for index {index}: {bias_results}\")\n",
    "        final_counter = Counter(bias_results)\n",
    "        final_bias, _ = final_counter.most_common(1)[0]\n",
    "        df.at[index, 'label'] = 1 if final_bias == \"biased\" else 0\n",
    "    \n",
    "    # Set placeholders for additional analysis\n",
    "    df[\"topic\"] = \"XXX\"\n",
    "    df[\"type\"] = \"YYY\"\n",
    "    df[\"predicted\"] = \"ZZZ\"\n",
    "    \n",
    "    # Save the cleaned dataset\n",
    "    df.to_csv(\"./clean_datasets/BIG_TESTING_DATAFRAME.csv\", index=False)\n",
    "    print(\"Data processing complete and saved.\")\n",
    "\n",
    "IMPORT_BABEv3()\n",
    "IMPORT_ALL_THE_NEWS_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_test(data, assessing):\n",
    "    # Getting unique items from the assessing column to initialize scores dictionary\n",
    "    unique_items = data[assessing].unique().tolist()\n",
    "    scores = {item: {\"correct\": 0, \"count\": 0} for item in unique_items}\n",
    "    for _, row in data.iterrows():\n",
    "        correct = False\n",
    "        if pd.isna(row['type']) and row['label'] == 0:\n",
    "            correct = True\n",
    "        elif row['type'] in ['left', 'right', 'center'] and row['label'] == 1:\n",
    "            correct = True\n",
    "        item = row[assessing]\n",
    "        scores[item][\"count\"] += 1\n",
    "        if correct:\n",
    "            scores[item][\"correct\"] += 1\n",
    "    for item in scores:\n",
    "        scores[item]['score'] = scores[item]['correct'] / scores[item]['count']\n",
    "    scores = pd.DataFrame(scores).T\n",
    "    scores[\"assesing\"] = assessing #could do with changing from field\n",
    "    scores = pd.DataFrame(scores).T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FIN_TRAIN_D4DATA(training_data_path):\n",
    "    # Load and preprocess the dataset\n",
    "    df = pd.read_csv(training_data_path)\n",
    "    df = df[['text', 'label']]\n",
    "    df_train, df_val = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    val_dataset = Dataset.from_pandas(df_val)\n",
    "\n",
    "    # Load the tokenizer and model configuration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\")\n",
    "    config = AutoConfig.from_pretrained(\"d4data/bias-detection-model\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\", config=config)\n",
    "\n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Convert to TensorFlow datasets\n",
    "    train_dataset = tokenized_train_dataset.to_tf_dataset(\n",
    "        columns=['input_ids', 'attention_mask', 'label'],\n",
    "        shuffle=True,\n",
    "        batch_size=16,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    val_dataset = tokenized_val_dataset.to_tf_dataset(\n",
    "        columns=['input_ids', 'attention_mask', 'label'],\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    # Define the training arguments and compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained('./fine_tuned/D4DATA')\n",
    "    tokenizer.save_pretrained('./fine_tuned/D4DATA')\n",
    "\n",
    "\n",
    "#code for fine-tuning individual models\n",
    "#FIN_TRAIN_D4DATA(\"./clean_datasets/TRAINING_DATAFRAME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FIN_TRAIN_D1V1DE(training_data_path):\n",
    "    # Load and preprocess the dataset\n",
    "    df = pd.read_csv(training_data_path)\n",
    "    df = df[['text', 'label']]\n",
    "    df_train, df_val = train_test_split(df, test_size=0.1)\n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    val_dataset = Dataset.from_pandas(df_val)\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    # Load the pretrained model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"D1V1DE/bias-detection\")\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10\n",
    "    )\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset\n",
    "    )\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    # Save the fine-tuned model\n",
    "    tokenizer.save_pretrained(\"./fine_tuned/D1V1DE\")\n",
    "    model.save_pretrained(\"./fine_tuned/D1V1DE\")\n",
    "\n",
    "\n",
    "#code for fine-tuning individual models\n",
    "#FIN_TRAIN_D1V1DE(\"./clean_datasets/TRAINING_DATAFRAME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FIN_TRAIN_VALURANK(training_data_path):\n",
    "    # Load and preprocess the dataset\n",
    "    df = pd.read_csv(training_data_path)\n",
    "    df = df[['text', 'label']]\n",
    "    df_train, df_val = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(df_train)\n",
    "    val_dataset = Dataset.from_pandas(df_val)\n",
    "\n",
    "    # Load the tokenizer and model configuration\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"valurank/distilroberta-bias\")\n",
    "    config = AutoConfig.from_pretrained(\"valurank/distilroberta-bias\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"valurank/distilroberta-bias\", config=config, from_pt=True)  # Added from_pt=True here\n",
    "\n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Convert to TensorFlow datasets\n",
    "    train_dataset = tokenized_train_dataset.to_tf_dataset(\n",
    "        columns=['input_ids', 'attention_mask', 'label'],\n",
    "        shuffle=True,\n",
    "        batch_size=16,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
    "    )\n",
    "\n",
    "    val_dataset = tokenized_val_dataset.to_tf_dataset(\n",
    "        columns=['input_ids', 'attention_mask', 'label'],\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
    "    )\n",
    "\n",
    "    # Define the training arguments and compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained('./fine_tuned/VALURANK')\n",
    "    tokenizer.save_pretrained('./fine_tuned/VALURANK')\n",
    "\n",
    "# Example usage:\n",
    "#FIN_TRAIN_VALURANK(\"./clean_datasets/TRAINING_DATAFRAME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GEN_TEST_BIAS(model,CSV):\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    try:\n",
    "        pipe = pipeline(\"text-classification\", model=model)\n",
    "        test_df = pd.read_csv(CSV)\n",
    "        max_length = 300  # Define maximum length for model input\n",
    "        for index, row in test_df.iterrows():\n",
    "            text_data = row['text']\n",
    "            # Chunk the text first before tokenizing\n",
    "            words = text_data.split()\n",
    "            chunk_size = int(max_length / 2)  # Rough estimate of chunk size in words\n",
    "            chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "            chunk_predictions = []\n",
    "            for chunk in chunks:\n",
    "                # Tokenize and then convert to string within the model's token limit\n",
    "                result = pipe(chunk)\n",
    "                if result:\n",
    "                    chunk_predictions.append(result[0].get(\"label\", \"No Label\"))\n",
    "            if chunk_predictions:\n",
    "                most_common = max(set(chunk_predictions), key=chunk_predictions.count)\n",
    "                test_df.at[index, 'Predicted'] = most_common\n",
    "            else:\n",
    "                test_df.at[index, 'Predicted'] = \"No Prediction\"\n",
    "        test_df.to_csv(\"temp.csv\", index = False)\n",
    "        # Analyze results by type and topic\n",
    "        Type = temp_test(test_df, \"type\")\n",
    "        Topic = temp_test(test_df, \"topic\")\n",
    "        # Combine results and calculate scores\n",
    "        # Modify the join line in your function\n",
    "        results = Type.join(Topic).T\n",
    "        results['score'] = results['score'] * 100\n",
    "        # Save final results to a CSV file\n",
    "        print(\"eval competed succesfully\")\n",
    "        return results\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError encountered: {e}\")\n",
    "        print(\"Contents of DataFrame:\")\n",
    "        print(results.head())\n",
    "\n",
    "def createAssesments(model_path, dataset_path):\n",
    "    df = GEN_TEST_BIAS(model_path, dataset_path)\n",
    "    df['model'] = model_path\n",
    "    df['dataset'] = dataset_path\n",
    "    df[\"assessed\"] = df.index\n",
    "    return df\n",
    "\n",
    "# Using the function to generate and label dataframes\n",
    "dataframes = []\n",
    "dataframes.append(createAssesments(\"valurank/distilroberta-bias\", \"./clean_datasets/TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"valurank/distilroberta-bias\", \"./clean_datasets/BIG_TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"./fine_tuned/VALURANK\", \"./clean_datasets/TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"./fine_tuned/VALURANK\", \"./clean_datasets/BIG_TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"D1V1DE/bias-detection\", \"./clean_datasets/TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"D1V1DE/bias-detection\", \"./clean_datasets/BIG_TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"./fine_tuned/D1V1DE\", \"./clean_datasets/TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"./fine_tuned/D1V1DE\", \"./clean_datasets/BIG_TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"d4data/bias-detection-model\", \"./clean_datasets/BIG_TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"d4data/bias-detection-model\", \"./clean_datasets/TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"./fine_tuned/D4DATA\", \"./clean_datasets/TESTING_DATAFRAME.csv\"))\n",
    "dataframes.append(createAssesments(\"./fine_tuned/D4DATA\", \"./clean_datasets/BIG_TESTING_DATAFRAME.csv\"))\n",
    "\n",
    "# Combining the dataframes into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "combined_df.fillna(\"none\", inplace=True)\n",
    "# Saving the combined dataframe to a CSV file\n",
    "combined_df.to_csv(\"Model_Results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_test_bias(model,csv):\n",
    "    pipe = pipeline(\"text-classification\", model = model)\n",
    "    data = pd.read_csv(csv)\n",
    "    for index, row in data.iterrows():\n",
    "        predicted = pipe(row[\"text\"])\n",
    "        predicted = predicted[0][\"label\"]\n",
    "        if predicted.lower() == \"biased\":\n",
    "            guess = 1\n",
    "        else:\n",
    "            guess = 0\n",
    "        data.at[index, 'Predicted'] = guess\n",
    "    return data\n",
    "\n",
    "new_test_bias(\"D1V1DE/bias-detection\", \"./clean_datasets/TESTING_DATAFRAME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GEN_EVAL_evaluate_model(model, tokenizer, eval_dataset):\n",
    "    #used for evaluating performance of models in terms of speed. needs further research\n",
    "    # Initialize the trainer\n",
    "    trainer = Trainer(model=model)\n",
    "    # Tokenize the evaluation dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate(tokenized_eval_dataset)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GEN_VIS_star(data,key):\n",
    "    FilteredData = data[data['field'] == key]\n",
    "    # Preparing the data for the star plot (radar chart)\n",
    "    FilteredData = FilteredData.reset_index()\n",
    "    FilteredData['index'] = FilteredData['index'].fillna('No Bias')\n",
    "    labels=FilteredData['index']\n",
    "    #print(labels)\n",
    "    stats=FilteredData['score']\n",
    "    #print(stats)\n",
    "\n",
    "    # Create radar chart\n",
    "    angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()\n",
    "    stats=np.concatenate((stats,[stats[0]]))\n",
    "    angles+=angles[:1]\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "    ax.fill(angles, stats, color='blue', alpha=0.25)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.title('Star Plot of '+  str(key) +' vs. accuracy rating')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
